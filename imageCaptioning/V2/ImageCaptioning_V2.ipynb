{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyKyNARaDwb0"
      },
      "source": [
        "# **Version 2**\n",
        "----\n",
        "Model with image encoder and language encoder.\n",
        "\n",
        "Image encoder is the features extracted from a pre-trained model.\n",
        "\n",
        "Language encoder is built using GRUs, Attention layers for the partial text of the caption.\n",
        "\n",
        "The features are then merged to predict the next word in for the caption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kShtQqHtD2h-"
      },
      "source": [
        "#### Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYPKbQryDm0y"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hphmgqSpEIRT"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import string\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Add, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50V2, VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr4Fc4QMGa53",
        "outputId": "9360db5b-d065-4300-c58f-5c7363f1471f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip3 install pipreqsnb\n",
        "!pipreqsnb --savepath 'requirements.txt' '/content/drive/My Drive/Colab Notebooks/ImageCaptioning_V2.ipynb'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pipreqsnb\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/99/1fd7d0ce621dd5491e9e0086cdd84223a9729fe9ae76202758e2e444c70f/pipreqsnb-0.2.2.tar.gz\n",
            "Collecting pipreqs\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/83/b1560948400a07ec094a15c2f64587b70e1a5ab5f7b375ba902fcab5b6c3/pipreqs-0.4.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from pipreqs->pipreqsnb) (0.6.2)\n",
            "Collecting yarg\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/90/89a2ff242ccab6a24fbab18dbbabc67c51a6f0ed01f9a0f41689dc177419/yarg-0.1.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from yarg->pipreqs->pipreqsnb) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->pipreqsnb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->pipreqsnb) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->pipreqsnb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->yarg->pipreqs->pipreqsnb) (1.24.3)\n",
            "Building wheels for collected packages: pipreqsnb\n",
            "  Building wheel for pipreqsnb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pipreqsnb: filename=pipreqsnb-0.2.2-cp36-none-any.whl size=3989 sha256=141293f7c3d3f25ac1f13c36f8e9b40bf9defdc4e8e13b8dbbb0ba90243eeb02\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/48/eb/c365e598808484772b5285721af9252665e29a20dbae98182a\n",
            "Successfully built pipreqsnb\n",
            "Installing collected packages: yarg, pipreqs, pipreqsnb\n",
            "Successfully installed pipreqs-0.4.10 pipreqsnb-0.2.2 yarg-0.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0laSFDcJBs4"
      },
      "source": [
        "#### Loading the annotations file, pre-processing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azOLtQF9HH67"
      },
      "source": [
        "main_dir = '/content/drive/My Drive/ImageCaptioning/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cIpl-SJGWu"
      },
      "source": [
        "#### LOADING ANNOTATIONS FILE, PREPROCESSING IT AND SAVING IT AS 'DESCRIPTIONS.TXT\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.strip().split('\\t')\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = image_desc\n",
        "\treturn mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\tdesc = desc.split()\n",
        "\t\tdesc = [word.lower() for word in desc]\n",
        "\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\tdescriptions[key] =  ' '.join(desc)\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_doc(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\tlines.append(key + '\\t' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "filename = main_dir + 'annotations_ajio_v4_full(2).txt'\n",
        "doc = load_doc(filename)\n",
        "print('Finished loading', filename)\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "clean_descriptions(descriptions)\n",
        "print(\"Finished cleaning descriptions\")\n",
        "all_tokens = ' '.join(descriptions.values()).split()\n",
        "vocabulary = set(all_tokens)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "#save_doc(descriptions, main_dir+'descriptions_v2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU0924ocJQ1H"
      },
      "source": [
        "#### Shuffling and splitting the dataset to training, validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA831UkoJQYA"
      },
      "source": [
        "product_ids = list(descriptions.keys())\n",
        "random.shuffle(product_ids)\n",
        "train_product_ids = product_ids[:int(0.9*len(product_ids))]\n",
        "random.shuffle(train_product_ids)\n",
        "val_product_ids = product_ids[int(0.9*len(product_ids)):int(0.95*len(product_ids))]\n",
        "random.shuffle(val_product_ids)\n",
        "test_product_ids = product_ids[int(0.95*len(product_ids)):]\n",
        "random.shuffle(test_product_ids)\n",
        "print(len(product_ids))\n",
        "print(len(train_product_ids))\n",
        "print(len(val_product_ids))\n",
        "print(len(test_product_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgRtFgAmJcUb"
      },
      "source": [
        "#### Pre-processing to find vocab_size and max_caption_length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8iEBo6RJjgA"
      },
      "source": [
        "### PREPROCESSING CAPTIONS FOR TRAINING\n",
        "def load_captions(descriptions,train_product_ids):\n",
        "    train_captions=[]\n",
        "    for image_id in descriptions.keys():\n",
        "      if image_id in train_product_ids:\n",
        "        train_captions.append('startseq '+descriptions[image_id]+' endseq')\n",
        "    \n",
        "    return train_captions\n",
        "\n",
        "train_captions = load_captions(descriptions,train_product_ids)\n",
        "val_captions = load_captions(descriptions, val_product_ids)\n",
        "test_captions = load_captions(descriptions, test_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av1QKXEVJp8i"
      },
      "source": [
        "### WORD TO INDEX DICTIONARY\n",
        "\n",
        "corpus = []\n",
        "for caption in val_captions+train_captions+test_captions:\n",
        "    for token in caption.split():\n",
        "        corpus.append(token)\n",
        "        \n",
        "hash_map = Counter(corpus)\n",
        "vocab = []\n",
        "for token,count in hash_map.items():\n",
        "        if count > 1:\n",
        "            vocab.append(token)\n",
        "        \n",
        "print('Number of original tokens',len(hash_map))\n",
        "print('Number of tokens after threshold',len(vocab))\n",
        "\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "    \n",
        "for idx,token in enumerate(vocab):\n",
        "    word_to_index[token] = idx+1\n",
        "    index_to_word[idx+1] = token\n",
        "\n",
        "vocab_size = len(index_to_word) + 1 # one for appended 0's\n",
        "\n",
        "print(len(index_to_word))\n",
        "\n",
        "## MAX LENGTH OF CAPTIONS\n",
        "\n",
        "def max_len_caption(all_train_captions):   \n",
        "    max_len = 0\n",
        "    for caption in all_train_captions:\n",
        "        max_len = max(max_len,len(caption.split()))\n",
        "    print('Maximum length of caption= ',max_len)\n",
        "    return max_len\n",
        "\n",
        "max_length_caption = max_len_caption(train_captions+val_captions+test_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtxiQwCSJr1w"
      },
      "source": [
        "#### Extracting the image folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqU8OB7pJuXD"
      },
      "source": [
        "## UNZIPPING IMAGES\n",
        "from zipfile import ZipFile\n",
        "\n",
        "filename= main_dir+'images_v4_full(2) (1).zip'\n",
        "print(filename)\n",
        "zip = ZipFile(filename)\n",
        "zip.extractall()\n",
        "\n",
        "import os\n",
        "print(len(os.listdir('/content/images_v4_full(2)')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLcDZcjhJ3_f"
      },
      "source": [
        "#### Extracting the image features and store it in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVW6CdfeJ4Rf"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "\n",
        "def load_img_features(product_ids):\n",
        "\tfeatures=dict()\n",
        "\tproduct_ids_new = []\n",
        "\tmodel = InceptionV3(include_top=False, pooling='max')\n",
        "\timage_dir ='/content/images_v4_full(2)/'\n",
        "\n",
        "\tfor j,id in enumerate(product_ids): \n",
        "\t\tif j%100 == 0:\n",
        "\t\t\tprint(j)\n",
        "\t\ttry:\n",
        "\t\t\timage_name = image_dir+ id+'.jpg'\n",
        "\t\t\timage=  load_img(image_name,target_size=(299, 299,3))\n",
        "\t\t\timage = img_to_array(image)\n",
        "\t\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t\timage = preprocess_input(image)\n",
        "\t\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\t\tproduct_ids_new.append(id)\n",
        "\t\t\tfeatures[id] = feature.reshape(2,2,512)\n",
        "\t\texcept OSError:\n",
        "\t\t  print(\"Error with file\")\n",
        "  \n",
        "\tprint(\"Loaded\", len(features.keys()) ,\"number of features\" )\n",
        "\tprint(features[id].shape)\n",
        "\tprint(type(features[id]))\n",
        "\treturn features, product_ids_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIhZQjKyJ72u"
      },
      "source": [
        "train_features, train_product_ids = load_img_features(train_product_ids)\n",
        "val_features, val_product_ids = load_img_features(val_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x89vxHSQKVG9"
      },
      "source": [
        "#### Loading the captions dictionary\n",
        "\n",
        "Creating seperate dictionaries for different splits of data, along with the `<startseq>` and `<endseq>` token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_IpimCPKViD"
      },
      "source": [
        "def load_captions_dict(descriptions,train_product_ids):\n",
        "    train_captions=dict()\n",
        "    for image_id in descriptions.keys():\n",
        "      if image_id in train_product_ids:\n",
        "        train_captions[image_id]= 'startseq '+descriptions[image_id]+' endseq'\n",
        "    \n",
        "    return train_captions\n",
        "\n",
        "train_captions = load_captions_dict(descriptions,train_product_ids)\n",
        "val_captions = load_captions_dict(descriptions, val_product_ids)\n",
        "test_captions = load_captions_dict(descriptions, test_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql0eX5wRKjZx"
      },
      "source": [
        "#### Defining the Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeCetK8CKl6b"
      },
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        " \n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "                #'supports_masking':self.supports_masking,\n",
        "                #'init':self.init,\n",
        "                'W_regularizer': self.W_regularizer,\n",
        "                'b_regularizer': self.b_regularizer,\n",
        "                'W_constraint': self.W_constraint,\n",
        "                'b_constraint': self.b_constraint,\n",
        "                'bias': self.bias,\n",
        "                'step_dim':self.step_dim,\n",
        "                #'features_dim':self.features_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        a = K.exp(eij)\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7aXWJX5KnOp"
      },
      "source": [
        "#### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfYGa0bcKotL"
      },
      "source": [
        "ImageEncoderInput = Input(shape=(512,))\n",
        "ImageEncoder = Dropout(0.35)(ImageEncoderInput)\n",
        "ImageEncoder = Dense(256, activation='relu')(ImageEncoder)\n",
        "\n",
        "# Language Encoder\n",
        "LanguageEncoderInput = Input(shape=(max_length_caption,))\n",
        "LanguageEncoder = Embedding(vocab_size, 128, mask_zero=True)(LanguageEncoderInput)\n",
        "LanguageEncoder = Dropout(0.35)(LanguageEncoder)\n",
        "LanguageEncoder = Bidirectional(GRU(128, return_sequences=True, dropout=0.25,recurrent_dropout=0.25))(LanguageEncoder) \n",
        "LanguageEncoder = Attention(max_length_caption)(LanguageEncoder)\n",
        "\n",
        "# Decoder\n",
        "Decoder = Add()([ImageEncoder, LanguageEncoder])\n",
        "Decoder = Dense(500, activation='relu')(Decoder)\n",
        "FinalDecoder = Dense(vocab_size, activation='softmax')(Decoder)\n",
        "\n",
        "model = Model(inputs=[ImageEncoderInput, LanguageEncoderInput], outputs=FinalDecoder)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=5e-4,decay=1e-5))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etC0VqNjKzvW"
      },
      "source": [
        "#### Plotting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDSa4uKpK2R4"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1fe7UHLK40n"
      },
      "source": [
        "#### Defining the custom data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpCJNfk-K8En"
      },
      "source": [
        "def data_generator(descriptions, wordtoix, photos, categories, max_length, num_photos_per_batch):\n",
        "\n",
        "\timage_dir ='/content/images_v4_full(2)/'\n",
        "\t#in_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "\tX1, X2, y1, y2 = list(), list(), list(), list()\n",
        "\tn=0\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc in descriptions.items():\n",
        "\t\t\tn+=1\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\ttemp=main_dir\n",
        "\t\t\ttry:\n",
        "\t\t\t\tphoto = photos[key]\n",
        "\t\t\t\n",
        "\t\t\t\tfor abc in range(1):\n",
        "\t\t\t\t\t# encode the sequence\n",
        "\t\t\t\t\n",
        "\t\t\t\t\tseq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t# split one sequence into multiple X, y pairs\n",
        "\t\t\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t\t\t# split into input and output pair\n",
        "\t\t\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t\t\t# pad input sequence\n",
        "\t\t\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length, dtype='float64')[0]\n",
        "\t\t\t\t\t\t# encode output sequence\n",
        "\t\t\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t\t\t# store\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t\tX1.append(photo)\n",
        "\t\t\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\t\t\ty1.append(out_seq)\n",
        "\t\t\t\t\t\ty2.append(categories[key])\n",
        "\t\t\texcept KeyError:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# yield the batch data\n",
        "\t\t\tif n==num_photos_per_batch:\n",
        "\t\t\t\t\"\"\"\n",
        "\t\t\t\tyield ([np.array(X1, dtype='float64'),\n",
        "\t\t\t\t\t\t\tnp.array(X2, dtype='float64')], np.array(y1, dtype='float64'))\n",
        "\t\t\t\t\"\"\"\n",
        "\t\t\t\tyield ([np.array(X1, dtype='float64'),\n",
        "\t\t\t\t\t\t\tnp.array(X2, dtype='float64')], {'decoder_output':np.array(y1, dtype='float64'),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'classifier_output':np.array(y2, dtype='float64')})\n",
        "\t\t\t\t\n",
        "\t\t\t\t#X1, X2, y1, y2 = list(), list(), list(), list()\n",
        "\t\t\t\tX1, X2, y1, y2 = list(), list(), list(), list()\n",
        "\t\t\t\tn=0\n",
        "\t\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDrJ7vW5LJIH"
      },
      "source": [
        "#### Finally! Training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlNbE53aLLVO"
      },
      "source": [
        "epochs = 50\n",
        "number_pics_per_batch = 128\n",
        "steps = len(train_captions)//number_pics_per_batch\n",
        "temp = main_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buMOH3TcLObx"
      },
      "source": [
        "generator = data_generator(train_captions,  word_to_index, train_features ,train_categories, max_length_caption, number_pics_per_batch)\n",
        "val_generator = data_generator(val_captions, word_to_index, val_features, val_categories, max_length_caption, number_pics_per_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liCTnPt3LSwp"
      },
      "source": [
        "output_dir = main_dir + 'model2/'\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1,min_lr=1e-7, factor = 0.5)\n",
        "\n",
        "if os.path.exists(output_dir) == False:\n",
        "  os.mkdir(output_dir)\n",
        "  \n",
        "history = model.fit_generator(generator, validation_data = val_generator, \n",
        "                                    validation_steps = len(val_captions)//number_pics_per_batch\n",
        "                                    ,epochs=epochs,\n",
        "                                    #,epochs=1,\n",
        "                                steps_per_epoch=steps,\n",
        "                                verbose=1, \n",
        "                               callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG67ts71LV18"
      },
      "source": [
        "#### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCrrmi7QLXOg"
      },
      "source": [
        "model.save(output_dir+'V2(1).h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOzJZ01wLfBQ"
      },
      "source": [
        "#### Plotting the loss curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AWgyDCTLjic"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        " \n",
        "# summarize history for loss\n",
        "print('loss')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model total loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "print('decoder_output_loss')\n",
        "plt.plot(history.history['decoder_output_loss'])\n",
        "plt.plot(history.history['val_decoder_output_loss'])\n",
        "plt.title('model decoder loss')\n",
        "plt.ylabel('decoder loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "print('classifier_output_loss')\n",
        "plt.plot(history.history['classifier_output_loss'])\n",
        "plt.plot(history.history['val_classifier_output_loss'])\n",
        "plt.title('model classifier loss')\n",
        "plt.ylabel('classifier loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Ix2NxYLn85"
      },
      "source": [
        "#### Saving the corresponding mapping data in **JSON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ELYpJ78LnNJ"
      },
      "source": [
        "f= open(output_dir+'V2(1).json','w')\n",
        "data = {'word_to_index':word_to_index,\n",
        "        'index_to_word':index_to_word}\n",
        "json.dump(data,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuOL5IyRL77X"
      },
      "source": [
        "#### Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eL4u2Z8L9r6"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "output_dir = main_dir + 'model2/'\n",
        "\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "        model = load_model(output_dir+'V2(1).h5', custom_objects={'Attention':Attention})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nacD5P4rMDHM"
      },
      "source": [
        "#### Loading the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_BwM_p7MFE8"
      },
      "source": [
        "test_features, test_product_ids = load_img_features(test_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-JhnsApMHhA"
      },
      "source": [
        "#### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvFS8o_SMJBt"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def greedySearch(photo):\n",
        "  in_text = 'startseq'\n",
        "  for i in range(max_length_caption):\n",
        "    sequence = [word_to_index[w] for w in in_text.split(' ') if w in word_to_index]\n",
        "    sequence = pad_sequences([sequence], maxlen = max_length_caption)\n",
        "    yhat = model.predict([photo,sequence],verbose=1)\n",
        "    yhat = np.argmax(yhat,axis=0)\n",
        "    word = index_to_word[yhat]\n",
        "    in_text+=' '+word\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  final = in_text.split()\n",
        "  final = final[1:-1]\n",
        "  final = ' '.join(final)\n",
        "  return final\n",
        "\n",
        "print(len(test_product_ids))\n",
        "z=173\n",
        "pic= test_product_ids[z]\n",
        "print(pic)\n",
        "image = test_features[pic].reshape(1,2,2,512)\n",
        "cv2_imshow(cv2.imread('images_v4_full(2)/'+pic+'.jpg'))\n",
        "result = greedySearch(image)\n",
        "\n",
        "print(\"Actual output:\",test_captions[pic])\n",
        "print(\"Predicted output:\",result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}