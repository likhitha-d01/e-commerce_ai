{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning_V4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxJ4KMdMcIO6"
      },
      "source": [
        "# **Version 4**\n",
        "----\n",
        "Image Captioning model with category prediction.\n",
        "\n",
        "Model with image encoder and language encoder.\n",
        "\n",
        "Image encoder is the features extracted from a pre-trained model, with additional convolutional layers. \n",
        "\n",
        "Language encoder is built using GRUs, Attention layers for the partial text of the caption.\n",
        "\n",
        "The features are then merged to predict the next word in for the caption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH4Nfb4EcY2K"
      },
      "source": [
        "#### Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgFj9Y1gb9xf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j2Iu3CWcczQ"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import string\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Add, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50V2, VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl7Y7hAncsL-"
      },
      "source": [
        "main_dir = '/content/drive/My Drive/ImageCaptioning/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091bhgJBcgcM"
      },
      "source": [
        "#### Creating the requirements.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRcYfvPgceWg"
      },
      "source": [
        "!pip3 install pipreqsnb\n",
        "!pipreqsnb --savepath 'requirements.txt' '/content/drive/My Drive/Colab Notebooks/ImageCaptioning_V3.ipynb'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezBnrmgJdnQ1"
      },
      "source": [
        "#### Loading the annotations file, pre-processing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-BHR1-ddpT0"
      },
      "source": [
        "# loading the annotations file\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\tcat_mapping = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.strip().split('\\t')\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\timage_id, image_desc, image_cat = tokens[0], tokens[1], tokens[2]\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\timage_desc = image_desc\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = image_desc\n",
        "\t\t\tcat_mapping[image_id] = image_cat\n",
        "\treturn mapping, cat_mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\tdesc = desc.split(' ')\n",
        "\t\tdesc = [word.lower() for word in desc]\n",
        "\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\tdescriptions[key] =  ' '.join(desc)\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_doc(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\tlines.append(key + '\\t' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "filename = main_dir + 'annotations_ajio_v4_full(2).txt'\n",
        "doc = load_doc(filename)\n",
        "print('Finished loading', filename)\n",
        "descriptions, categories = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "clean_descriptions(descriptions)\n",
        "print(\"Finished cleaning descriptions\")\n",
        "all_tokens = ' '.join(descriptions.values()).split()\n",
        "vocabulary = set(all_tokens)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "unq_categories = set(categories.values())\n",
        "print('Number of categories: %d' % len(unq_categories))\n",
        "#save_doc(descriptions, main_dir+'descriptions_v2.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATiGDRzwdxzd"
      },
      "source": [
        "#### Shuffling and splitting the data to training, validation and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Px8qc5d313"
      },
      "source": [
        "product_ids = list(descriptions.keys())\n",
        "random.shuffle(product_ids)\n",
        "train_product_ids = product_ids[:int(0.9*len(product_ids))]\n",
        "random.shuffle(train_product_ids)\n",
        "val_product_ids = product_ids[int(0.9*len(product_ids)):int(0.95*len(product_ids))]\n",
        "random.shuffle(val_product_ids)\n",
        "test_product_ids = product_ids[int(0.95*len(product_ids)):]\n",
        "random.shuffle(test_product_ids)\n",
        "print(len(product_ids))\n",
        "print(len(train_product_ids))\n",
        "print(len(val_product_ids))\n",
        "print(len(test_product_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-1mmXy1d7Ka"
      },
      "source": [
        "#### Pre-processing to find vocab_size and max_caption_length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au4EqRyWd7kM"
      },
      "source": [
        "### PREPROCESSING CAPTIONS FOR TRAINING\n",
        "def load_captions(descriptions,train_product_ids):\n",
        "    train_captions=[]\n",
        "    for image_id in descriptions.keys():\n",
        "      if image_id in train_product_ids:\n",
        "        train_captions.append('startseq '+descriptions[image_id]+' endseq')\n",
        "    \n",
        "    return train_captions\n",
        "\n",
        "captions = load_captions(descriptions, product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chPyiXP_d8ui"
      },
      "source": [
        "# WORD TO INDEX DICTIONARY\n",
        "\n",
        "corpus = []\n",
        "for caption in captions:\n",
        "    for token in caption.split():\n",
        "        corpus.append(token)\n",
        "        \n",
        "hash_map = Counter(corpus)\n",
        "vocab = []\n",
        "for token,count in hash_map.items():\n",
        "        if count > 1:\n",
        "            vocab.append(token)\n",
        "        \n",
        "print('Number of original tokens',len(hash_map))\n",
        "print('Number of tokens after threshold',len(vocab))\n",
        "\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "    \n",
        "for idx,token in enumerate(vocab):\n",
        "    word_to_index[token] = idx+1\n",
        "    index_to_word[idx+1] = token\n",
        "\n",
        "vocab_size = len(index_to_word) + 1 # one for appended 0's\n",
        "\n",
        "print(len(index_to_word))\n",
        "\n",
        "## MAX LENGTH OF CAPTIONS\n",
        "\n",
        "def max_len_caption(all_train_captions):   \n",
        "    max_len = 0\n",
        "    for caption in all_train_captions:\n",
        "        max_len = max(max_len,len(caption.split()))\n",
        "    print('Maximum length of caption= ',max_len)\n",
        "    return max_len\n",
        "\n",
        "max_length_caption = max_len_caption( captions )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRBJ1RIAd_fv"
      },
      "source": [
        "#### Extracting the image folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SU1xG9OeBPZ"
      },
      "source": [
        "## UNZIPPING IMAGES\n",
        "from zipfile import ZipFile\n",
        "\n",
        "filename= main_dir+'images_v4_full(2) (1).zip'\n",
        "print(filename)\n",
        "zip = ZipFile(filename)\n",
        "zip.extractall()\n",
        "\n",
        "import os\n",
        "print(len(os.listdir('/content/images_v4_full(2)')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6BISyKkeDZ0"
      },
      "source": [
        "#### Extracting the image features and store it in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n-fHra6eGBv"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "\n",
        "def load_img_features(product_ids):\n",
        "\tfeatures=dict()\n",
        "\tproduct_ids_new = []\n",
        "\tmodel = InceptionV3(include_top=False, pooling='max')\n",
        "\timage_dir ='/content/images_v4_full(2)/'\n",
        "\n",
        "\tfor j,id in enumerate(product_ids): \n",
        "\t\tif j%100 == 0:\n",
        "\t\t\tprint(j)\n",
        "\t\ttry:\n",
        "\t\t\timage_name = image_dir+ id+'.jpg'\n",
        "\t\t\timage=  load_img(image_name,target_size=(299, 299,3))\n",
        "\t\t\timage = img_to_array(image)\n",
        "\t\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t\timage = preprocess_input(image)\n",
        "\t\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\t\tproduct_ids_new.append(id)\n",
        "\t\t\tfeatures[id] = feature.reshape(2,2,512)\n",
        "\t\texcept OSError:\n",
        "\t\t  print(\"Error with file\")\n",
        "  \n",
        "\tprint(\"Loaded\", len(features.keys()) ,\"number of features\" )\n",
        "\tprint(features[id].shape)\n",
        "\tprint(type(features[id]))\n",
        "\treturn features, product_ids_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCsmL-CreIax"
      },
      "source": [
        "train_features, train_product_ids = load_img_features(train_product_ids)\n",
        "val_features, val_product_ids = load_img_features(val_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5b1oHULeKUf"
      },
      "source": [
        "#### Loading the captions dictionary\n",
        "\n",
        "Creating seperate dictionaries for different splits of data, along with the `<startseq>` and `<endseq>` token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-aka73aeKsd"
      },
      "source": [
        "def load_captions_dict(descriptions,train_product_ids):\n",
        "    train_captions=dict()\n",
        "    for image_id in descriptions.keys():\n",
        "      if image_id in train_product_ids:\n",
        "        train_captions[image_id]= 'startseq '+descriptions[image_id]+' endseq'\n",
        "    \n",
        "    return train_captions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvLRe-QXxPdK"
      },
      "source": [
        "train_captions = load_captions_dict(descriptions,train_product_ids)\n",
        "val_captions = load_captions_dict(descriptions, val_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA44ktF6ePUm"
      },
      "source": [
        "#### Pre-processing categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuBDC3nXeRLz"
      },
      "source": [
        "# PREPROCESSING CATEGORIES\n",
        "import pandas as pd\n",
        "\n",
        "encoded_data, mapping_index = pd.Series(list(unq_categories)).factorize()\n",
        "\n",
        "def oneHotEncoding(x):\n",
        "    ans = np.zeros((13))\n",
        "    ans[x] = 1\n",
        "    return ans\n",
        "\n",
        "def load_categories_dict(categories, train_product_ids):\n",
        "\ttrain_categories=dict()\n",
        "\tfor image_id in categories.keys():\n",
        "\t\tif image_id in train_product_ids:\n",
        "\t\t\tx = mapping_index.get_loc(categories[image_id])\n",
        "\t\t\ttrain_categories[image_id] = oneHotEncoding(x)\n",
        "\treturn train_categories\n",
        "\n",
        "train_categories = load_categories_dict(categories, train_product_ids)\n",
        "val_categories = load_categories_dict(categories, val_product_ids)\n",
        "test_categories = load_categories_dict(categories, test_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gohKuugo4SdU"
      },
      "source": [
        "#### Defining the Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekjMfmAZ4Xzz"
      },
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        " \n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "                #'supports_masking':self.supports_masking,\n",
        "                #'init':self.init,\n",
        "                'W_regularizer': self.W_regularizer,\n",
        "                'b_regularizer': self.b_regularizer,\n",
        "                'W_constraint': self.W_constraint,\n",
        "                'b_constraint': self.b_constraint,\n",
        "                'bias': self.bias,\n",
        "                'step_dim':self.step_dim,\n",
        "                #'features_dim':self.features_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        a = K.exp(eij)\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTfCHJYaeatG"
      },
      "source": [
        "#### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSp0Pgguedp3"
      },
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, TimeDistributed\n",
        "\n",
        "ImageInput =Input(shape=(2,2,512,))\n",
        "ImageEncoder = Conv2D(512,(3,3),padding='same',activation='relu')(ImageInput)\n",
        "ImageEncoder = MaxPooling2D((2,2))(ImageEncoder)\n",
        "ImageEncoder = Conv2D(512,(3,3),padding='same',activation='relu')(ImageEncoder)\n",
        "ImageEncoder = Conv2D(512,(1,1),padding='same',activation='relu')(ImageEncoder)\n",
        "ImageEncoder = Flatten()(ImageEncoder)\n",
        "ImageEncoder = Dropout(0.4)(ImageEncoder)\n",
        "ImageEncoder = Dense(1024, activation='relu')(ImageEncoder)\n",
        "ImageEncoder = Dense(256, activation='relu')(ImageEncoder)\n",
        "Classifier = Dense(13, activation='softmax', name='classifier_output')(ImageEncoder)\n",
        "\n",
        "# Language Encoder\n",
        "LanguageEncoderInput = Input(shape=(max_length_caption,))\n",
        "LanguageEncoder = Embedding(vocab_size, 128, mask_zero=True)(LanguageEncoderInput)\n",
        "LanguageEncoder = Dropout(0.35)(LanguageEncoder)\n",
        "LanguageEncoder = Bidirectional(GRU(128, return_sequences=True, dropout=0.25))(LanguageEncoder) \n",
        "LanguageEncoder = Bidirectional(GRU(128, return_sequences=True, dropout=0.25))(LanguageEncoder) \n",
        "LanguageEncoder = Attention(max_length_caption)(LanguageEncoder)\n",
        "\n",
        "#Decoder\n",
        "Decoder = Add()([ImageEncoder, LanguageEncoder])\n",
        "Decoder = Reshape((1,256))(Decoder)\n",
        "Decoder = Bidirectional(GRU(128, return_sequences=True,dropout=0.25))(Decoder) \n",
        "Decoder = Flatten()(Decoder)\n",
        "Decoder = Dropout(0.4)(Decoder)\n",
        "Decoder = Dense(1024, activation='relu')(Decoder)\n",
        "Decoder = Dropout(0.4)(Decoder)\n",
        "Decoder = Dense(500, activation='relu')(Decoder)\n",
        "\n",
        "FinalDecoder = Dense(vocab_size, activation='softmax', name='decoder_output')(Decoder)\n",
        "losses = {\n",
        "\t\"classifier_output\": \"categorical_crossentropy\",\n",
        "\t\"decoder_output\": \"categorical_crossentropy\",\n",
        "}\n",
        "\n",
        "lossWeights = {\"classifier_output\": 0.5, \"decoder_output\": 1.5}\n",
        "\n",
        "model = Model(inputs=[ImageInput, LanguageEncoderInput], outputs=[FinalDecoder, Classifier], name='Captioner')\n",
        "model.compile(loss=losses, loss_weights = lossWeights, optimizer=Adam(lr=5e-5,decay=1e-5))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6QlYq7sek6e"
      },
      "source": [
        "#### Plotting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4KNLcyMenMR"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LsotQ10ep74"
      },
      "source": [
        "#### Defining the custom data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeUew1Vpeujb"
      },
      "source": [
        "def data_generator(descriptions, wordtoix, photos, categories, max_length, num_photos_per_batch):\n",
        "\n",
        "\timage_dir ='/content/images_v4_full(2)/'\n",
        "\t#in_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "\tX1, X2, y1, y2 = list(), list(), list(), list()\n",
        "\tn=0\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc in descriptions.items():\n",
        "\t\t\tn+=1\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\ttemp=main_dir\n",
        "\t\t\ttry:\n",
        "\t\t\t\tphoto = photos[key]\n",
        "\t\t\t\n",
        "\t\t\t\tfor abc in range(1):\n",
        "\t\t\t\t\t\t# encode the sequence\n",
        "\t\t\t\t\n",
        "\t\t\t\t\tseq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t# split one sequence into multiple X, y pairs\n",
        "\t\t\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t\t\t# split into input and output pair\n",
        "\t\t\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t\t\t# pad input sequence\n",
        "\t\t\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length, dtype='float64')[0]\n",
        "\t\t\t\t\t\t# encode output sequence\n",
        "\t\t\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t\t\t# store\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t\tX1.append(photo)\n",
        "\t\t\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\t\t\ty1.append(out_seq)\n",
        "\t\t\t\t\t\ty2.append(categories[key])\n",
        "\t\t\texcept KeyError:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t# yield the batch data\n",
        "\t\t\tif n==num_photos_per_batch:\n",
        "\t\t\t\tyield ([np.array(X1, dtype='float64'),\n",
        "\t\t\t\t\t\t\tnp.array(X2, dtype='float64')], {'decoder_output':np.array(y1, dtype='float64'),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'classifier_output':np.array(y2, dtype='float64')})\n",
        "\t\t\t\t\n",
        "\t\t\t\t#X1, X2, y1, y2 = list(), list(), list(), list()\n",
        "\t\t\t\tX1, X2, y1, y2 = list(), list(), list(), list()\n",
        "\t\t\t\tn=0\n",
        "\t\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR5vx6E_e0l2"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ia9yPo5e2D_"
      },
      "source": [
        "epochs = 50\n",
        "number_pics_per_batch = 128\n",
        "output_dir = main_dir + 'model4/'\n",
        "temp = main_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGnLKxve3qH"
      },
      "source": [
        "generator = data_generator(train_captions,  word_to_index, train_features ,train_categories, max_length_caption, number_pics_per_batch)\n",
        "val_generator = data_generator(val_captions, word_to_index, val_features, val_categories, max_length_caption, number_pics_per_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Itx04oe5K0"
      },
      "source": [
        "steps = len(train_captions)//number_pics_per_batch\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1,min_lr=1e-7, factor = 0.5)\n",
        "\n",
        "if os.path.exists(output_dir) == False:\n",
        "  os.mkdir(output_dir)\n",
        "  \n",
        "history = model4.fit_generator(generator, validation_data = val_generator, \n",
        "                                    validation_steps = len(val_captions)//number_pics_per_batch\n",
        "                                    ,epochs=epochs,\n",
        "                                steps_per_epoch=steps,\n",
        "                                verbose=1, \n",
        "                               callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXPDc8Y9e9rQ"
      },
      "source": [
        "#### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uML2IQnBe_Cp"
      },
      "source": [
        "model.save(output_dir+'V4(1).h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fte_Lk9nfD_U"
      },
      "source": [
        "#### Plotting the loss curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnJeD_xKfG6a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        " \n",
        "# summarize history for loss\n",
        "print('loss')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model total loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "print('decoder_output_loss')\n",
        "plt.plot(history.history['decoder_output_loss'])\n",
        "plt.plot(history.history['val_decoder_output_loss'])\n",
        "plt.title('model decoder loss')\n",
        "plt.ylabel('decoder loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "print('classifier_output_loss')\n",
        "plt.plot(history.history['classifier_output_loss'])\n",
        "plt.plot(history.history['val_classifier_output_loss'])\n",
        "plt.title('model classifier loss')\n",
        "plt.ylabel('classifier loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4gx9VbfJRs"
      },
      "source": [
        "#### Saving the corresponding data in a **JSON** file for using it while loading the mdoel later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK5GsiVzfK4s"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "encoded_data, mapping_index = pd.Series(list(unq_categories)).factorize()\n",
        "print(mapping_index)\n",
        "\n",
        "Categories_mapping = list(mapping_index)\n",
        "print(Categories_mapping)\n",
        "\n",
        "f= open(output_dir+'V4(1).json','w')\n",
        "data = {'word_to_index':word_to_index,\n",
        "        'index_to_word':index_to_word,\n",
        "        'categories_mapping':Categories_mapping}\n",
        "json.dump(data,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni9-_bDIfQl4"
      },
      "source": [
        "#### Loading the saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iwzt0-XfScO"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "output_dir = main_dir + 'model4/'\n",
        "\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "    model = load_model(output_dir+'final_model.h5', custom_objects={'Attention':Attention})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qomGHaKTmS6E"
      },
      "source": [
        "f = open(output_dir + 'final_model.json','r')\n",
        "data = json.load(f)\n",
        "index_to_word = data['index_to_word']\n",
        "word_to_index = data['word_to_index']\n",
        "categories_mapping = data['categories_mapping']\n",
        "\n",
        "max_length_caption = 13\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OaHyf1JfZRL"
      },
      "source": [
        "#### Loading the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHIaHWyfa6m"
      },
      "source": [
        "test_features, test_product_ids = load_img_features(test_product_ids)\n",
        "test_captions = load_captions_dict(descriptions, test_product_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP_VUoCBfdmb"
      },
      "source": [
        "#### Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3v9RZI8xvAY"
      },
      "source": [
        "##### Testing the model **without beam search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J5xV1kjxy6a"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def greedySearch(photo):\n",
        "    final = 'startseq'\n",
        "\n",
        "    for i in range(max_length_caption):\n",
        "        sequence = [word_to_index[w] for w in final.split(' ') if w in word_to_index]\n",
        "        sequence = pad_sequences([sequence], maxlen = max_length_caption)\n",
        "        yhat = model.predict([photo,sequence],verbose=1)\n",
        "        seq_yhat, cls_yhat = yhat\n",
        "        seq_yhat = str(np.argmax(seq_yhat[0], axis=0))\n",
        "        cls_yhat+=cls_yhat\n",
        "           \n",
        "        word = index_to_word[seq_yhat]\n",
        "        final_results[j]+=' '+word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    cls_yhat = cls_yhat / (i+1)\n",
        "    cls_yhat = np.argmax(cls_yhat)\n",
        "    print(categories_mapping[cls_yhat])\n",
        "    final = final.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    \n",
        "    return final_results\n",
        "\n",
        "test_samples = 30\n",
        "for i in range(test_samples):\n",
        "        pic= test_product_ids[i]\n",
        "        print(pic)\n",
        "        image = test_features[pic].reshape(1,2,2,512)\n",
        "        cv2_imshow(cv2.imread('images_v4_full(2)/'+pic+'.jpg'))\n",
        "\n",
        "        result = greedySearch(image)\n",
        "\n",
        "        print(\"Actual output:\",test_captions[pic])\n",
        "        print(\"Predicted output:\",result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxf22ssBwaYc"
      },
      "source": [
        "##### Testing the model **with beam search V1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aYPInPzfif6"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def greedySearch(photo, beam_k = 5):\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    final_results=[]\n",
        "\n",
        "    for _ in range(beam_k):\n",
        "        final_results.append(in_text)\n",
        "\n",
        "    for j in range(beam_k):\n",
        "        for i in range(max_length_caption):\n",
        "            sequence = [word_to_index[w] for w in final_results[j].split(' ') if w in word_to_index]\n",
        "            sequence = pad_sequences([sequence], maxlen = max_length_caption)\n",
        "            yhat = model.predict([photo,sequence],verbose=1)\n",
        "            seq_yhat, cls_yhat = yhat\n",
        "            seq_yhat = str(seq_yhat.argsort()[0][-(j+1)])\n",
        "            cls_yhat+=cls_yhat\n",
        "            \n",
        "            word = index_to_word[seq_yhat]\n",
        "            final_results[j]+=' '+word\n",
        "            if word == 'endseq':\n",
        "                break\n",
        "        cls_yhat = cls_yhat / (i+1)\n",
        "        cls_yhat = np.argmax(cls_yhat)\n",
        "        print(categories_mapping[cls_yhat])\n",
        "        final = final_results[j].split()\n",
        "        final = final[1:-1]\n",
        "        final_results[j] = ' '.join(final)\n",
        "    return final_results\n",
        "\n",
        "test_samples = 30\n",
        "for i in range(test_samples):\n",
        "        pic= test_product_ids[i]\n",
        "        print(pic)\n",
        "        image = test_features[pic].reshape(1,2,2,512)\n",
        "        cv2_imshow(cv2.imread('images_v4_full(2)/'+pic+'.jpg'))\n",
        "\n",
        "        beam_k = 5\n",
        "        result = greedySearch(image, beam_k)\n",
        "\n",
        "        print(\"Actual output:\",test_captions[pic])\n",
        "        print(\"Predicted output:\",result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh9hjvcWwf3N"
      },
      "source": [
        "##### Testing the model **with beam search V2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl0VQXKSwkHl"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def greedySearch(photo, beam_k = 5):\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    final_results=[]\n",
        "\n",
        "    for _ in range(beam_k):\n",
        "        final_results.append(in_text)\n",
        "\n",
        "    for i in range(max_length_caption):\n",
        "        sequence = [word_to_index[w] for w in in_text.split(' ') if w in word_to_index]\n",
        "        sequence = pad_sequences([sequence], maxlen = max_length_caption)\n",
        "        yhat = model.predict([photo,sequence],verbose=0)\n",
        "        seq_yhat, cls_yhat = yhat\n",
        "        seq_yhat = seq_yhat.argsort()[0][-beam_k:]\n",
        "        cls_yhat+=cls_yhat\n",
        "\n",
        "        in_text += ' ' + index_to_word[str(seq_yhat[-1])]\n",
        "\n",
        "        for idx in range(beam_k):\n",
        "\n",
        "            word = index_to_word[str(seq_yhat[idx])]\n",
        "            if word != 'endseq':\n",
        "                final_results[idx]+=' '+word\n",
        "    cls_yhat = cls_yhat / (i+1)\n",
        "    cls_yhat = np.argmax(cls_yhat)\n",
        "    print(categories_mapping[cls_yhat])\n",
        "\n",
        "    for j in range(beam_k):\n",
        "        final = final_results[j].split()\n",
        "        final = final[1:-1]\n",
        "        final_results[j] = ' '.join(final)\n",
        "    return final_results\n",
        "\n",
        "test_samples = 30\n",
        "for i in range(test_samples):\n",
        "    pic= test_product_ids[i]\n",
        "    print(pic)\n",
        "    image = test_features[pic].reshape(1,2,2,512)\n",
        "    cv2_imshow(cv2.imread('images_v4_full(2)/'+pic+'.jpg'))\n",
        "\n",
        "    beam_k = 5\n",
        "    result = greedySearch(image, beam_k)\n",
        "\n",
        "    print(\\\"Actual output:\",test_captions[pic])\n",
        "    print(\\\"Predicted output:\",result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-deSKG5M5b"
      },
      "source": [
        "##### Testing the model on external dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4UDwpr2kNPS"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "def greedy_search(image_path ):\n",
        "\n",
        "    image_model = InceptionV3(include_top=False, pooling='max')\n",
        "\n",
        "    image=  load_img(image_path,target_size=(299, 299,3))\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    image = preprocess_input(image)\n",
        "    feature = image_model.predict(image, verbose=0)\n",
        "    feature = feature.reshape(-1, 2, 2, 512)\n",
        "\n",
        "    final = 'startseq'\n",
        "    for i in range(max_length_caption):\n",
        "        sequence = [word_to_index[w] for w in final.split(' ') if w in word_to_index]\n",
        "        sequence = pad_sequences([sequence], maxlen = max_length_caption)\n",
        "        yhat = model.predict([feature,sequence],verbose=0)\n",
        "        seq_yhat, cls_yhat = yhat\n",
        "        print(seq_yhat.shape)\n",
        "        seq_yhat = str(np.argmax(seq_yhat[0],axis=0))\n",
        "        cls_yhat+=cls_yhat\n",
        "            \n",
        "        word = index_to_word[seq_yhat]\n",
        "        final += ' '+ word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    cls_yhat = cls_yhat / (i+1)\n",
        "    cls_yhat = np.argmax(cls_yhat)\n",
        "    print(categories_mapping[cls_yhat])\n",
        "    final = final.split()\n",
        "    final = final[1:-1]\n",
        "    final= ' '.join(final)\n",
        "\n",
        "    return final\n",
        "\n",
        "folder = os.path.join(main_dir, 'try_captioning')\n",
        "\n",
        "for actor in ['actor5', 'actor6', 'actor10']:\n",
        "    actor_path = os.path.join(folder, actor)\n",
        "\n",
        "    for image in sorted(os.listdir(actor_path)):\n",
        "\n",
        "        image_path = os.path.join(actor_path, image)\n",
        "        cv2_imshow(cv2.resize(cv2.imread(image_path), (300, 300)))\n",
        "        print(actor)\n",
        "        print(image)\n",
        "        caption = greedy_search(image_path)\n",
        "        print(\"Predicted caption: \", caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3GGCFB2eZta"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}