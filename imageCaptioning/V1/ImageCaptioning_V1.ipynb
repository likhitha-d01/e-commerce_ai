{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZEPko-r-Y06"
   },
   "source": [
    "# **Version 1**\n",
    "---\n",
    "A simple image captioning model using pre-trained embeddings to embed the words in the model. \n",
    "The image encoder is a pre-trained model, and the features of the image are extracted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZicK7PI-gpb"
   },
   "source": [
    "#### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTDFqSrOECr8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfDhMaly-Ntn"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    " \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, LSTM, Add, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "hOG-mVqlHrQG",
    "outputId": "710d9efa-f1ee-4514-8bea-54c46abe2a81"
   },
   "outputs": [],
   "source": [
    "!pip3 install pipreqsnb\n",
    "!pipreqsnb --savepath 'requirements.txt' '/content/drive/My Drive/Colab Notebooks/ImageCaptioning_V1.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QArop0Y-lLH"
   },
   "outputs": [],
   "source": [
    "main_dir = '/content/drive/My Drive/ImageCaptioning/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXbxJ-RW-nlw"
   },
   "source": [
    "#### Loading the annotations file, pre-processing it and saving it as a descriptions.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "BQ0NKxEk-q7O",
    "outputId": "f00df350-08c0-4e54-8cf1-49bf0506edd4"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    function to read the annotations file.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    filename: str\n",
    "        full path of the annptations file\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    \"\"\"\n",
    "    function to create a mapping of the caption with the filename\n",
    "    \"\"\"\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = image_desc\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    \"\"\"\n",
    "    function to clean the descriptions.\n",
    "    \"\"\"\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        desc = desc.split()\n",
    "        desc = [word.lower() for word in desc]\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        descriptions[key] =  ' '.join(desc)\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_doc(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + '\\t' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "filename = main_dir + 'annotations_ajio_v3.txt'\n",
    "doc = load_doc(filename)\n",
    "print('Finished loading annotations.txt')\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "clean_descriptions(descriptions)\n",
    "print(\"Finished cleaning descriptions\")\n",
    "all_tokens = ' '.join(descriptions.values()).split()\n",
    "vocabulary = set(all_tokens)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "#save_doc(descriptions, main_dir+'descriptions_v2_m1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVIKRyTZ-270"
   },
   "source": [
    "#### Splitting the dataset\n",
    "\n",
    "Splitting the dataset as training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIUYL0uq_DoP"
   },
   "outputs": [],
   "source": [
    "### SPLITTING DATASET\n",
    "import random\n",
    "product_ids = list(descriptions.keys())\n",
    "random.shuffle(product_ids)\n",
    "train_product_ids = product_ids[:int(0.8*len(product_ids))]\n",
    "val_product_ids = product_ids[int(0.9*len(product_ids)):int(0.95*len(product_ids))]\n",
    "test_product_ids = product_ids[int(0.95*len(product_ids)):]\n",
    "print(len(product_ids))\n",
    "print(len(train_product_ids))\n",
    "print(len(val_product_ids))\n",
    "print(len(test_product_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h94RQxn_ID2"
   },
   "source": [
    "#### Some pre-processing to find the vocab_size, max_length_of_caption for the model\n",
    "\n",
    "`vocab_size` is required for building the model and deciding the size of the final layer of the model\n",
    "\n",
    "`max_length_of_caption` is required to understand the number of times the prediction loop has to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTpdOCbM_IaG"
   },
   "outputs": [],
   "source": [
    "### PREPROCESSING CAPTIONS FOR TRAINING\n",
    "def load_captions(descriptions,train_product_ids):\n",
    "    train_captions=[]\n",
    "    for image_id in descriptions.keys():\n",
    "      if image_id in train_product_ids:\n",
    "        train_captions.append('startseq '+descriptions[image_id]+' endseq')\n",
    "    \n",
    "    return train_captions\n",
    "\n",
    "train_captions = load_captions(descriptions,train_product_ids)\n",
    "val_captions = load_captions(descriptions, val_product_ids)\n",
    "test_captions = load_captions(descriptions, test_product_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO8LgyJA_Kha"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for caption in val_captions+train_captions:\n",
    "    for token in caption.split():\n",
    "        corpus.append(token)\n",
    "        \n",
    "hash_map = Counter(corpus)\n",
    "vocab = []\n",
    "for token,count in hash_map.items():\n",
    "        vocab.append(token)\n",
    "        \n",
    "print('Number of original tokens',len(hash_map))\n",
    "print('Number of tokens after threshold',len(vocab))\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "    \n",
    "for idx,token in enumerate(vocab):\n",
    "    word_to_index[token] = idx+1\n",
    "    index_to_word[idx+1] = token\n",
    "\n",
    "vocab_size = len(index_to_word) + 1 # one for appended 0's\n",
    "\n",
    "## max length of train captions\n",
    "\n",
    "def max_len_caption(all_train_captions):   \n",
    "    max_len = 0\n",
    "    for caption in all_train_captions:\n",
    "        max_len = max(max_len,len(caption.split()))\n",
    "    print('Maximum length of caption= ',max_len)\n",
    "    return max_len\n",
    "\n",
    "max_length_caption = max_len_caption(train_captions+val_captions+test_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujnjA9-x_Vzo"
   },
   "source": [
    "#### Loading the GloVe embeddings for the model\n",
    "\n",
    "We are using pre-trained embeddings to embed the words in our captions in the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaasleJc_REc"
   },
   "outputs": [],
   "source": [
    "### LOADING GLOVE EMBEDDINGS\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(main_dir+'glove.6B.50d.txt', encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K46udDA8_eG0"
   },
   "outputs": [],
   "source": [
    "## Embedding matrix\n",
    "embedding_dim = 50\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxYsSJDV_fNY"
   },
   "source": [
    "#### Extracting the zip folder of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSeh02bT_hif"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zip = ZipFile(main_dir+'/images_v3.zip',mode='r')\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUVguWBa_jjh"
   },
   "source": [
    "#### Extracting the images features using VGG-16 and storing it in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH9kvjkN_i5u"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_img_features(product_ids):\n",
    "    features=dict()\n",
    "    product_ids_new = []\n",
    "    image_dir ='images_v3/'\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    for j,id in enumerate(product_ids): \n",
    "        print(j)\n",
    "        try:\n",
    "            image_name = image_dir+id+'.jpg'\n",
    "            image=  load_img(image_name,target_size=(224, 224,3))\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            product_ids_new.append(id)\n",
    "            features[id] = feature.reshape(7,7,512)\n",
    "        except OSError:\n",
    "            print(\"Error with file\")\n",
    "  \n",
    "    print(\"Loaded\", len(features.keys()) ,\"number of features\" )\n",
    "    print(features[id].shape)\n",
    "    print(type(features[id]))\n",
    "    return features, product_ids_new\n",
    "\n",
    "train_features, train_product_ids = load_img_features(train_product_ids)\n",
    "val_features, val_product_ids = load_img_features(val_product_ids)\n",
    "test_features, test_product_ids = load_img_features(test_product_ids)\n",
    "\n",
    "print(train_features[train_product_ids[0]].flatten().reshape(-1,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZWqzWNj_qOs"
   },
   "source": [
    "#### Loading the captions dictionary\n",
    "\n",
    "Creating seperate dictionaries for different splits of data, along with the `<startseq>` and `<endseq>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjAdOsqD_sax"
   },
   "outputs": [],
   "source": [
    "def load_captions_dict(descriptions,train_product_ids):\n",
    "    train_captions=dict()\n",
    "    for image_id in descriptions.keys():\n",
    "        if image_id in train_product_ids:\n",
    "            train_captions[image_id]= 'startseq '+descriptions[image_id]+' endseq'\n",
    "    \n",
    "    return train_captions\n",
    "\n",
    "train_captions = load_captions_dict(descriptions,train_product_ids)\n",
    "val_captions = load_captions_dict(descriptions, val_product_ids)\n",
    "test_captions = load_captions_dict(descriptions, test_product_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUoOwURf_0F5"
   },
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w66MeB7UAALR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 7, 7, 512)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 5, 5, 3)      13827       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 2, 2, 3)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 13, 50)       200000      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2, 2, 3)      0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 13, 50)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2, 2, 256)    1024        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          314368      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2, 2, 256)    0           dense_2[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2, 2, 256)    65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4000)         4100000     flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,695,011\n",
      "Trainable params: 4,695,011\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=(7,7,512,))\n",
    "conv1 = Conv2D(filters = 3, kernel_size=(3,3))(inputs1)\n",
    "pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "fe1 = Dropout(0.4)(pool1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length_caption,))\n",
    "se1 = Embedding(vocab_size,50, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = Add()([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "flatten2 = Flatten()(decoder2)\n",
    "outputs = Dense(vocab_size, activation='softmax')(flatten2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZALwNhj-AEXl"
   },
   "source": [
    "#### Plotting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKA5JJFlAHEi"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file=main_dir+'model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhxfdFq1AI19"
   },
   "source": [
    "#### Setting the GloVe embeddings matrix as the embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S31TNWSbAMfD"
   },
   "outputs": [],
   "source": [
    "print(model.layers[4])\n",
    "\n",
    "model.layers[4].set_weights([embedding_matrix])\n",
    "model.layers[4].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU0KHp2lARQV"
   },
   "source": [
    "#### Defining the custom data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zqhYx7hATvE"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            temp=main_dir\n",
    "            \n",
    "            photo = photos[key]\n",
    "      \n",
    "            for abc in range(1):\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                \n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length, dtype='float64')[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "             \n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "                 \n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[np.array(X1, dtype='float64'),\n",
    "                        np.array(X2, dtype='float64')], \n",
    "                       np.array(y, dtype='float64')]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ReigXBSAWUh"
   },
   "source": [
    "#### Finally! Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgeM5LwdAZWb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=5e-4,decay=1e-5))\n",
    "epochs = 10\n",
    "number_pics_per_batch = 16\n",
    "steps = len(train_captions)//number_pics_per_batch\n",
    "temp = main_dir\n",
    "\n",
    "generator = data_generator(train_captions, train_features, word_to_index, max_length_caption, number_pics_per_batch)\n",
    "val_generator = data_generator(val_captions, val_features,  word_to_index, max_length_caption, number_pics_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ToHAlGrAcVb"
   },
   "outputs": [],
   "source": [
    "output_dir = main_dir + 'model1/'\n",
    "if os.path.exists(output_dir) == False:\n",
    "  os.mkdir(output_dir)\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(min_lr = 1e-7, monitor='val_loss', patience = 1, factor = 0.5, verbose=1)\n",
    "\n",
    "history = model.fit_generator(generator, validation_data=val_generator, validation_steps=8,\n",
    "                                   epochs=5, steps_per_epoch=steps, verbose=1,callbacks=[reduce_lr],shuffle=True)\n",
    "model.save(output_dir+'ajio_v3_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBC5v7HoAdvA"
   },
   "source": [
    "#### Loading the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOc2wdM6Af5C"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "output_dir = main_dir + 'model1/'\n",
    "model = load_model(output_dir+'ajio_v3.h5')\n",
    "\n",
    "print(test_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDlz90XZAlZx"
   },
   "source": [
    "#### Testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcZNWrNGAklV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def greedySearch(photo):\n",
    "  in_text = 'startseq'\n",
    "  for i in range(max_length_caption):\n",
    "    sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "    sequence = pad_sequences([sequence], maxlen = max_length_caption)\n",
    "    yhat = model.predict([photo,sequence],verbose=1)\n",
    "    yhat = np.argmax(yhat)\n",
    "    word = index_to_word[yhat]\n",
    "    in_text+=' '+word\n",
    "    if word == 'endseq':\n",
    "      break\n",
    "  final = in_text.split()\n",
    "  final = final[1:-1]\n",
    "  final = ' '.join(final)\n",
    "  return final\n",
    "\n",
    "print(len(test_product_ids))\n",
    "z=54\n",
    "pic= test_product_ids[z]\n",
    "print(pic)\n",
    "image = test_features[pic].reshape(1,7,7,512)\n",
    "plt.imshow(cv2.imread('images_v3/'+pic+'.jpg'))\n",
    "plt.show()\n",
    "result = greedySearch(image)\n",
    "print(\"Actual output:\",test_captions[pic])\n",
    "print(\"Predicted output:\",result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ImageCaptioning_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
